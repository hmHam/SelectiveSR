{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3fdeafa",
   "metadata": {},
   "source": [
    "### 単射となる変換のみを考える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "148c807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from settings.shift_funcs import get_funcs\n",
    "import numpy as np\n",
    "\n",
    "SEED = 0\n",
    "SAMPLE_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7040f90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augument_data(source):\n",
    "    data = []\n",
    "    for c in source:\n",
    "        new = [\n",
    "            c,\n",
    "            np.rot90(c, np.random.randint(1, 4)),\n",
    "            np.fliplr(c),\n",
    "            np.flipud(c)                \n",
    "        ]\n",
    "        data.extend(new)\n",
    "    data = np.array(data, dtype=source.dtype)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa6e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(original_set, filter_indices_set, funcs):  \n",
    "    dataset = []\n",
    "    for k, indices in enumerate(filter_indices_set):\n",
    "        img = original_set[k]\n",
    "        det_img = img.copy()\n",
    "        for i in indices:\n",
    "            func, _ = funcs[i]\n",
    "            det_img = func(det_img)\n",
    "        dataset.append(det_img)\n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6880b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataset(func_indices, shift_len, C=3, fname='', seed=SEED, size=SAMPLE_SIZE):\n",
    "    funcs = [get_funcs(*delta) for delta in [(shift_len, shift_len), (shift_len, 0), (0, shift_len)]]\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    train_dataset = datasets.MNIST(root='../open_data/', train=True, download=True)\n",
    "    idx = np.random.choice(train_dataset.data.shape[0], size)\n",
    "    imgs = train_dataset.data[idx].numpy()\n",
    "    augumented_dataset = augument_data(imgs)\n",
    "\n",
    "    train_filter_set = np.random.choice(func_indices, (augumented_dataset.shape[0], C))\n",
    "    train_dataset = apply(augumented_dataset, train_filter_set, funcs)\n",
    "\n",
    "    np.savez(\n",
    "        os.path.join('data', fname),\n",
    "        train_dataset=train_dataset,\n",
    "        train_func_labels=train_filter_set,\n",
    "        original_dataset=augumented_dataset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8bc7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータ\n",
    "def create_test_dataset(shift_len, fname=''):\n",
    "    funcs = [get_funcs(*delta) for delta in [(shift_len, shift_len), (shift_len, 0), (0, shift_len)]]\n",
    "    \n",
    "    np.random.seed(SEED + 1000)\n",
    "    test_dataset = datasets.MNIST(root='../open_data/', train=False, download=True).data\n",
    "    test_filter_set = np.random.choice([0, 1, 2], (test_dataset.shape[0], 3))\n",
    "    test_dataset = apply(test_dataset.numpy(), test_filter_set, funcs)\n",
    "    np.savez(\n",
    "        os.path.join('data', fname),\n",
    "        test_dataset=test_dataset,\n",
    "        test_func_labels=test_filter_set,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f775b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(shift_len):\n",
    "    os.makedirs('data/shift%d' % shift_len, exist_ok=True)\n",
    "    create_test_dataset(shift_len=SHIFT_LEN, fname='shift%d/test_dataset.npz' % shift_len)\n",
    "\n",
    "    create_train_dataset(func_indices=[0], shift_len=SHIFT_LEN, fname='shift%d/diag_dataset.npz' % shift_len)\n",
    "    create_train_dataset(func_indices=[0, 2], shift_len=SHIFT_LEN, fname='shift%d/diag_vert_dataset.npz' % shift_len)\n",
    "    create_train_dataset(func_indices=[0, 1], shift_len=SHIFT_LEN, fname='shift%d/diag_hori_dataset.npz' % shift_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cce287",
   "metadata": {},
   "source": [
    "### 1. シフト1のデータセットを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d732990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(shift_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241ab4c",
   "metadata": {},
   "source": [
    "### 2. シフト2のデータセットを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c674a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(shift_len=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b566f426",
   "metadata": {},
   "source": [
    "### 2. シフト3のデータセットを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e364d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(shift_len=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
